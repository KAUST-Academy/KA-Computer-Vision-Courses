{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPMhboEGrA+vGo3DvHBclDJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from IPython.display import clear_output"],"metadata":{"id":"Q_JRnxYlBS1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install torch\n","%pip install torchvision\n","%pip install matplotlib\n","\n","%pip install vit-pytorch\n","\n","clear_output()"],"metadata":{"id":"_Jqv5iAnBJNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","from vit_pytorch import SimpleViT\n","\n","from tqdm import tqdm"],"metadata":{"id":"31RMbEJvjP8E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Contents:\n","\n","We'll make a classifier for CIFAR10 dataset in pytorch using Vision Transformers (ViT).\n","\n","Note: instead of using the full scale ViT (which is very large), we will use a library called vit-pytorch to build a smaller model based on vit architecture.\n","\n","About CIFAR10:\n","\n","The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n","![CIFAR-10 image](https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png)"],"metadata":{"id":"_MsQqWtrB0vV"}},{"cell_type":"markdown","source":["## Loading the datasets and data loaders"],"metadata":{"id":"_MquRgv1DGlS"}},{"cell_type":"code","source":["# Define your data transformations\n","transform = transforms.Compose([\n","    transforms.Resize(224),  # ViT models expect 224x224 images\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n","])\n","\n","# Load CIFAR-100 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Define dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx6kBXdwjUKL","executionInfo":{"status":"ok","timestamp":1709844666987,"user_tz":-300,"elapsed":6250,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"0f34dc6f-c442-4b6f-e45c-e3857ba53985"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 94840203.49it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["## Defining the model"],"metadata":{"id":"5tky7DoDDPA7"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"sB9HTDQnC0Ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define your model\n","model = SimpleViT(\n","    image_size = 224,\n","    patch_size = 32,\n","    num_classes = 10,\n","    dim = 512,\n","    depth = 2,\n","    heads = 4,\n","    mlp_dim = 2048\n",")\n","\n","model.to(device)\n","\n","train_losses = []\n","val_losses = []"],"metadata":{"id":"w-EN1O_YjW3Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can also use the full scale model if you have the resources to train and experiment with it.\n","from torchvision import models as vision_models\n","\n","model = vision_models.vit_b_32(weights='DEFAULT')  # weights='DEFAULT' downloads the pretrained model weights (trained on imagenet data.)\n","\n","model.to(device)\n","\n","# note: we need to change the model's last layer because the original vit classifies b/w 1000 classes and we only have 10 classes for ciphar 10\n","\n","train_losses = []\n","val_losses = []"],"metadata":{"id":"voDb7f7vEFw-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"gr5n7y0_jb_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FWPr8IWBBBbl","outputId":"48e1d58a-6f5e-4a98-e3da-e4f1fe841439","executionInfo":{"status":"ok","timestamp":1709845907157,"user_tz":-300,"elapsed":1239096,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10: 100%|██████████| 196/196 [01:47<00:00,  1.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Train Loss: 1.8632, Train Accuracy: 31.89%, Test Accuracy: 39.75%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10: 100%|██████████| 196/196 [01:44<00:00,  1.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/10], Train Loss: 1.5628, Train Accuracy: 43.16%, Test Accuracy: 45.35%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10: 100%|██████████| 196/196 [01:45<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/10], Train Loss: 1.4289, Train Accuracy: 47.88%, Test Accuracy: 49.44%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10: 100%|██████████| 196/196 [01:44<00:00,  1.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/10], Train Loss: 1.3248, Train Accuracy: 52.17%, Test Accuracy: 53.88%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10: 100%|██████████| 196/196 [01:45<00:00,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/10], Train Loss: 1.2317, Train Accuracy: 55.66%, Test Accuracy: 55.02%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10: 100%|██████████| 196/196 [01:45<00:00,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/10], Train Loss: 1.1650, Train Accuracy: 57.95%, Test Accuracy: 56.01%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10: 100%|██████████| 196/196 [01:45<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/10], Train Loss: 1.1044, Train Accuracy: 60.44%, Test Accuracy: 60.58%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10: 100%|██████████| 196/196 [01:45<00:00,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/10], Train Loss: 1.0434, Train Accuracy: 62.62%, Test Accuracy: 60.52%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10: 100%|██████████| 196/196 [01:45<00:00,  1.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/10], Train Loss: 0.9984, Train Accuracy: 64.18%, Test Accuracy: 62.48%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 196/196 [01:45<00:00,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/10], Train Loss: 0.9567, Train Accuracy: 65.91%, Test Accuracy: 64.28%\n"]}],"source":["num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss = running_loss / len(train_loader)\n","    train_accuracy = 100. * correct / total\n","\n","    # Evaluate the model\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    test_accuracy = 100. * correct / total\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n","          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n","          f\"Test Accuracy: {test_accuracy:.2f}%\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"A2OAa2vnBjIA"},"execution_count":null,"outputs":[]}]}
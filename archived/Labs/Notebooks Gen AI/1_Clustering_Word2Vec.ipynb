{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLROfD2BhLB72kJpHI7SgM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Content\n","\n","In this notebook, we will see a Clustering Algorithm in action.\n","\n","We will start with an embedding model and some test words. we'll use the embedding model to convert the words into vectors and then we will use K-means algo to cluster those words/vectors.\n","\n","We will use Word2Vec for our embedding model\n","\n","We will implement KMeans and use SKLearn's AgglomerativeClustering and DBSCAN"],"metadata":{"id":"buQBa5oyYu5g"}},{"cell_type":"code","source":["from IPython.display import clear_output"],"metadata":{"id":"ErRKu9tQ63p4","executionInfo":{"status":"ok","timestamp":1722467312080,"user_tz":-300,"elapsed":641,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# %pip install gdown==4.5\n","\n","clear_output()"],"metadata":{"id":"zJvsnvQo62lu","executionInfo":{"status":"ok","timestamp":1722467312742,"user_tz":-300,"elapsed":1,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","import numpy as np\n","import gensim\n","\n","from tqdm import tqdm\n","\n","from sklearn.cluster import AgglomerativeClustering, DBSCAN\n","from sklearn.metrics import pairwise_distances_argmin_min"],"metadata":{"id":"Z5ia04-uuqPq","executionInfo":{"status":"ok","timestamp":1722467316009,"user_tz":-300,"elapsed":3268,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Downloading the embedding model"],"metadata":{"id":"7BeGFRsuTVhm"}},{"cell_type":"code","source":["!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM"],"metadata":{"id":"4cgFAjvDQhnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will take some time.\n","model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"],"metadata":{"id":"f2bbutk5utfP","executionInfo":{"status":"ok","timestamp":1722467387085,"user_tz":-300,"elapsed":66305,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Implementing the K-Means Clustering Algorithm"],"metadata":{"id":"DUvmEa-s8NIq"}},{"cell_type":"code","execution_count":58,"metadata":{"id":"K_MbTKCwthV2","executionInfo":{"status":"ok","timestamp":1722467010107,"user_tz":-300,"elapsed":451,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"outputs":[],"source":["# We will make the class api(function names and such) similar to Sklearn's\n","class KMeans:\n","\n","    def __init__(self, n_clusters=3, max_iter=300, tol=1e-4, random_state=None):\n","        self.n_clusters = n_clusters\n","        self.max_iter = max_iter\n","        self.tol = tol\n","        self.random_state = random_state\n","        self.centers = None\n","\n","    def fit(self, X):\n","\n","        np.random.seed(self.random_state)\n","\n","        self.centers = self._initialize_centroids(X)\n","\n","        for _ in range(self.max_iter):\n","            labels, min_distances = self._assign_labels(X)\n","            new_centers = self._calculate_centers(X, labels)\n","\n","            # Check for convergence\n","            if np.all(np.linalg.norm(new_centers - self.centers, axis=1) < self.tol):\n","                break\n","            self.centers = new_centers\n","\n","    def _initialize_centroids(self, X):\n","        n_samples, _ = X.shape\n","        centroids = np.empty((self.n_clusters, X.shape[1]))\n","        centroid_idx = np.random.choice(n_samples)\n","        centroids[0] = X[centroid_idx]\n","\n","        for k in range(1, self.n_clusters):\n","            distances = np.min(np.linalg.norm(X[:, np.newaxis] - centroids[:k], axis=2), axis=1)\n","            prob = distances / distances.sum()\n","            centroid_idx = np.random.choice(n_samples, p=prob)\n","            centroids[k] = X[centroid_idx]\n","\n","        return centroids\n","\n","    def _assign_labels(self, X):\n","        labels, min_distances = pairwise_distances_argmin_min(X, self.centers)\n","        return labels, min_distances\n","\n","    def _calculate_centers(self, X, labels):\n","        new_centers = np.zeros((self.n_clusters, X.shape[1]))\n","        for i in range(self.n_clusters):\n","            new_centers[i] = X[labels == i].mean(axis=0)\n","        return new_centers\n","\n","    def predict(self, X):\n","        labels, _ = self._assign_labels(X)\n","        return labels\n"]},{"cell_type":"markdown","source":["## Clustering the data"],"metadata":{"id":"Yz0gQbj19QPE"}},{"cell_type":"code","source":["# Let's pick out a few random words which we will cluster\n","\n","test_words = [\n","    \"apple\", \"mango\", \"banana\", \"peach\", \"cherry\",\n","    \"monkey\", \"tiger\", \"lion\", \"deer\", \"dog\",\n","    \"london\", \"belgium\", \"beijing\", \"lahore\"\n","\n","]\n","\n","test_words = [word.lower() for word in test_words]\n","test_word_vectors = np.array([model[word] for word in test_words])"],"metadata":{"id":"HGCDPpaC85AC","executionInfo":{"status":"ok","timestamp":1722467048525,"user_tz":-300,"elapsed":446,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["## K-Means Clustering"],"metadata":{"id":"DitIuL9fluc7"}},{"cell_type":"code","source":["clustering_model = KMeans(n_clusters=3, tol=1e-2)\n","clustering_model.fit(test_word_vectors)"],"metadata":{"id":"Lg9lKdo3-EHU","executionInfo":{"status":"ok","timestamp":1722467090407,"user_tz":-300,"elapsed":473,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["assignments = clustering_model.predict(test_word_vectors)  # return an array of a cluster id, for each word(or word vector)"],"metadata":{"id":"mb7GqhQXE1zX","executionInfo":{"status":"ok","timestamp":1722467092262,"user_tz":-300,"elapsed":1,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["cluster_id_to_words = {}\n","\n","for word, cluster_id in zip(test_words, assignments):\n","\n","    if cluster_id not in cluster_id_to_words:\n","        cluster_id_to_words[cluster_id] = []\n","    cluster_id_to_words[cluster_id].append(word)\n","\n","for cluster_id in sorted(cluster_id_to_words):\n","    print(f\"Cluster {cluster_id}: {', '.join(cluster_id_to_words[cluster_id])}\")\n","    print('-'*30)"],"metadata":{"id":"hxUjWBjoE7SI","executionInfo":{"status":"ok","timestamp":1722467092705,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"927c0044-0ec2-4933-de00-e00bf0dac4da"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster 0: monkey, tiger, lion, deer, dog\n","------------------------------\n","Cluster 1: apple, mango, banana, peach, cherry\n","------------------------------\n","Cluster 2: london, belgium, beijing, lahore\n","------------------------------\n"]}]},{"cell_type":"markdown","source":["## Agglomerative Clustering"],"metadata":{"id":"23SZ9gU6lwoa"}},{"cell_type":"code","source":["clustering_model = AgglomerativeClustering(n_clusters=3)\n","clustering_model.fit(test_word_vectors)"],"metadata":{"id":"xrFsiIMsGmQi","executionInfo":{"status":"ok","timestamp":1722467097719,"user_tz":-300,"elapsed":521,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"31b112b9-9a24-4fd2-c479-c86251062609"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AgglomerativeClustering(n_clusters=3)"],"text/html":["<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AgglomerativeClustering(n_clusters=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AgglomerativeClustering</label><div class=\"sk-toggleable__content\"><pre>AgglomerativeClustering(n_clusters=3)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["assignments = clustering_model.labels_  # return an array of a cluster id, for each word(or word vector)"],"metadata":{"id":"3pD6Y8UWl5qT","executionInfo":{"status":"ok","timestamp":1722467099692,"user_tz":-300,"elapsed":3,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["cluster_id_to_words = {}\n","\n","for word, cluster_id in zip(test_words, assignments):\n","\n","    if cluster_id not in cluster_id_to_words:\n","        cluster_id_to_words[cluster_id] = []\n","    cluster_id_to_words[cluster_id].append(word)\n","\n","for cluster_id in sorted(cluster_id_to_words):\n","    print(f\"Cluster {cluster_id}: {', '.join(cluster_id_to_words[cluster_id])}\")\n","    print('-'*30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTKv9HnFl8LO","executionInfo":{"status":"ok","timestamp":1722467099692,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"ccd21520-4215-422a-f946-64975a4601ad"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster 0: monkey, tiger, lion, deer, dog\n","------------------------------\n","Cluster 1: apple, mango, banana, peach, cherry\n","------------------------------\n","Cluster 2: london, belgium, beijing, lahore\n","------------------------------\n"]}]},{"cell_type":"markdown","source":["## DBSCAN clustering"],"metadata":{"id":"drlfcicRmy2N"}},{"cell_type":"code","source":["# Normalize the vectors. This will help us by reducing the required \"eps\" value for good results\n","\n","test_word_vectors_normalized = test_word_vectors/np.linalg.norm(test_word_vectors, axis=-1, keepdims=True)"],"metadata":{"id":"vLjfrp8Ap4Q-","executionInfo":{"status":"ok","timestamp":1722467174037,"user_tz":-300,"elapsed":455,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["np.linalg.norm(test_word_vectors_normalized[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-gh2iELpqo3","executionInfo":{"status":"ok","timestamp":1722467174524,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"a8aaad5f-1883-436e-e867-6501eb470478"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.99999994"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["clustering_model = DBSCAN(eps=1.1, min_samples=2)  # try playing around with these params\n","assignments = clustering_model.fit_predict(test_word_vectors_normalized)"],"metadata":{"id":"akUsCXdYmg7j","executionInfo":{"status":"ok","timestamp":1722467177370,"user_tz":-300,"elapsed":448,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["assignments"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nu4Ly88wm6Fd","executionInfo":{"status":"ok","timestamp":1722467177843,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"02088b47-f164-4467-c2c0-3c66223fa5e6"},"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2])"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["cluster_id_to_words = {}\n","\n","for word, cluster_id in zip(test_words, assignments):\n","\n","    if cluster_id not in cluster_id_to_words:\n","        cluster_id_to_words[cluster_id] = []\n","    cluster_id_to_words[cluster_id].append(word)\n","\n","for cluster_id in sorted(cluster_id_to_words):\n","    print(f\"Cluster {cluster_id}: {', '.join(cluster_id_to_words[cluster_id])}\")\n","    print('-'*30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blPr76dAnGj9","executionInfo":{"status":"ok","timestamp":1722467180194,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"2194f837-2683-45bc-b021-6c18330d1dd4"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster 0: apple, mango, banana, peach, cherry\n","------------------------------\n","Cluster 1: monkey, tiger, lion, deer, dog\n","------------------------------\n","Cluster 2: london, belgium, beijing, lahore\n","------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uZ5lMph0nqz8"},"execution_count":null,"outputs":[]}]}
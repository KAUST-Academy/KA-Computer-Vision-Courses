{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-algeria",
   "metadata": {
    "papermill": {
     "duration": 0.017654,
     "end_time": "2022-04-19T13:11:47.061687",
     "exception": false,
     "start_time": "2022-04-19T13:11:47.044033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chief-lease",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T13:11:47.102529Z",
     "iopub.status.busy": "2022-04-19T13:11:47.101014Z",
     "iopub.status.idle": "2022-04-19T13:11:48.973443Z",
     "shell.execute_reply": "2022-04-19T13:11:48.973954Z",
     "shell.execute_reply.started": "2021-06-28T12:48:02.818924Z"
    },
    "papermill": {
     "duration": 1.89472,
     "end_time": "2022-04-19T13:11:48.974253",
     "exception": false,
     "start_time": "2022-04-19T13:11:47.079533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import VOCDetection\n",
    "from tqdm import tqdm\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "from collections import Counter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff1a10",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "In this notebook, you will create a Yolo Based object detection model.\n",
    "\n",
    "Use the Pascal VOC dataset\n",
    "\n",
    "After training show the performance of the model by visualizing some images and their predicted bounding box on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-numbers",
   "metadata": {
    "papermill": {
     "duration": 0.01803,
     "end_time": "2022-04-19T13:11:49.533185",
     "exception": false,
     "start_time": "2022-04-19T13:11:49.515155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e40547df",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_classes = {\n",
    "    \"aeroplane\": 0,\n",
    "    \"bicycle\": 1,\n",
    "    \"bird\": 2,\n",
    "    \"boat\": 3,\n",
    "    \"bottle\": 4,\n",
    "    \"bus\": 5,\n",
    "    \"car\": 6,\n",
    "    \"cat\": 7,\n",
    "    \"chair\": 8,\n",
    "    \"cow\": 9,\n",
    "    \"diningtable\": 10,\n",
    "    \"dog\": 11,\n",
    "    \"horse\": 12,\n",
    "    \"motorbike\": 13,\n",
    "    \"person\": 14,\n",
    "    \"pottedplant\": 15,\n",
    "    \"sheep\": 16,\n",
    "    \"sofa\": 17,\n",
    "    \"train\": 18,\n",
    "    \"tvmonitor\": 19,\n",
    "}\n",
    "\n",
    "#  Reverse of label to class id mapping. needed because the model predictions will be ids and we need to change it to label to visualize it.\n",
    "reverse_voc_classes = {v: k for k, v in voc_classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "specific-train",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T13:11:49.954506Z",
     "iopub.status.busy": "2022-04-19T13:11:49.953760Z",
     "iopub.status.idle": "2022-04-19T13:11:49.956133Z",
     "shell.execute_reply": "2022-04-19T13:11:49.956516Z",
     "shell.execute_reply.started": "2021-06-28T12:48:36.212583Z"
    },
    "papermill": {
     "duration": 0.041748,
     "end_time": "2022-04-19T13:11:49.956644",
     "exception": false,
     "start_time": "2022-04-19T13:11:49.914896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, year='2012', image_set='train', transform=None, target_transform=None):\n",
    "        self.voc = VOCDetection(root, year=year, image_set=image_set, transform=None, target_transform=None, download=True)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.cache = [-1]*len(self.voc)\n",
    "\n",
    "        self.S = 7\n",
    "        self.B = 2\n",
    "        self.C = 20\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if isinstance(self.cache[index], tuple):\n",
    "            res = self.cache[index]\n",
    "\n",
    "        else:\n",
    "\n",
    "            image, target = self.voc[index]\n",
    "\n",
    "            width, height = image.size\n",
    "\n",
    "            boxes = []\n",
    "\n",
    "            for obj in target['annotation']['object']:\n",
    "                xmin = float(obj['bndbox']['xmin']) / width\n",
    "                ymin = float(obj['bndbox']['ymin']) / height\n",
    "                xmax = float(obj['bndbox']['xmax']) / width\n",
    "                ymax = float(obj['bndbox']['ymax']) / height\n",
    "                label_class = voc_classes[obj['name']]\n",
    "\n",
    "                centerx = (xmax + xmin) / 2\n",
    "                centery = (ymax + ymin) / 2\n",
    "                boxwidth = xmax - xmin\n",
    "                boxheight = ymax - ymin\n",
    "\n",
    "                boxes.append([label_class, centerx, centery, boxwidth, boxheight])\n",
    "\n",
    "            # boxes = torch.tensor(boxes)\n",
    "            image = FT.to_tensor(image)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "            # Convert To Cells\n",
    "            label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "\n",
    "            for box in boxes:\n",
    "                class_label, x, y, width, height = box\n",
    "\n",
    "                # i,j represents the cell row and cell column\n",
    "                i, j = int(self.S * y), int(self.S * x)\n",
    "                x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "                \"\"\"\n",
    "                Calculating the width and height of cell of bounding box,\n",
    "                relative to the cell is done by the following, with\n",
    "                width as the example:\n",
    "                \n",
    "                width_pixels = (width*self.image_width)\n",
    "                cell_pixels = (self.image_width)\n",
    "                \n",
    "                Then to find the width relative to the cell is simply:\n",
    "                width_pixels/cell_pixels, simplification leads to the\n",
    "                formulas below.\n",
    "                \"\"\"\n",
    "                width_cell, height_cell = (\n",
    "                    width * self.S,\n",
    "                    height * self.S,\n",
    "                )\n",
    "\n",
    "                # format: 0-19: OHC, 20: conf, 21-30: Boxes\n",
    "\n",
    "                # If no object already found for specific cell i,j\n",
    "                # Note: This means we restrict to ONE object\n",
    "                # per cell!\n",
    "\n",
    "                box_idx = 0\n",
    "\n",
    "                if label_matrix[i, j, self.C] == 0:\n",
    "\n",
    "                    # Set one hot encoding for class_label\n",
    "                    label_matrix[i, j, class_label] = 1\n",
    "\n",
    "                    # Box coordinates\n",
    "                    bbox_truth = torch.tensor(\n",
    "                        [1.0, x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "\n",
    "                    box_start = self.C+(5*box_idx)\n",
    "\n",
    "                    label_matrix[i, j, box_start:box_start+len(bbox_truth)] = bbox_truth\n",
    "\n",
    "            res = (image, label_matrix)\n",
    "            self.cache[index] = res\n",
    "\n",
    "        image, label_matrix = res\n",
    "\n",
    "        height, width = image.shape[-2:]\n",
    "\n",
    "        x_shift = int((0.2 * random.random() - 0.1) * width)\n",
    "        y_shift = int((0.2 * random.random() - 0.1) * height)\n",
    "        scale = 1 + 0.2 * random.random()\n",
    "\n",
    "        image = FT.affine(image, angle=0.0, scale=scale, translate=(x_shift, y_shift), shear=0.0)\n",
    "        image = FT.adjust_hue(image, 0.2 * random.random() - 0.1)\n",
    "        image = FT.adjust_saturation(image, 0.2 * random.random() + 0.9)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448), antialias=None)\n",
    "])\n",
    "\n",
    "train_dataset = VOCDataset('data', year='2007', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-finish",
   "metadata": {
    "papermill": {
     "duration": 0.019006,
     "end_time": "2022-04-19T13:11:49.012292",
     "exception": false,
     "start_time": "2022-04-19T13:11:48.993286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Model Architecture**\n",
    "\n",
    "Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76d2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sitting-kernel",
   "metadata": {
    "papermill": {
     "duration": 0.017647,
     "end_time": "2022-04-19T13:11:50.084706",
     "exception": false,
     "start_time": "2022-04-19T13:11:50.067059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Model Training**\n",
    "\n",
    "Train the model on VOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71d8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc4eb92a",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461126b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-czech",
   "metadata": {
    "papermill": {
     "duration": 0.208398,
     "end_time": "2022-04-19T13:18:04.874449",
     "exception": false,
     "start_time": "2022-04-19T13:18:04.666051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 386.770309,
   "end_time": "2022-04-19T13:18:07.381740",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-19T13:11:40.611431",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

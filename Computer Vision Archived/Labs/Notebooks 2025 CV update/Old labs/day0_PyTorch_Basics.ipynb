{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch Introduction: Cats and Dogs Classification\n",
    "In this notebook, we'll build a simple CNN classifier to distinguish between cats and dogs. We'll cover:\n",
    "1. Loading and preprocessing data using custom Dataset class\n",
    "2. Building a CNN model\n",
    "3. Training and validation loops\n",
    "4. Visualizing results\n",
    "\n"
   ],
   "id": "93c3b97e7be56fb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Import Required Libraries",
   "id": "ca1dd61280347889"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import kagglehub\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Create Custom Dataset Class\n",
    " \n",
    "The Dataset class is responsible for:\n",
    "- Loading individual images\n",
    "- Assigning labels (0 for cats, 1 for dogs)\n",
    "- Applying transformations\n",
    "- Providing length information\n",
    "\n"
   ],
   "id": "f28a376fb93acf21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"marquis03/cats-and-dogs\")"
   ],
   "id": "ab9894b0bd4b5069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(os.listdir(path))",
   "id": "b34e957f0994f60e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "\n",
    "        csv_file = os.path.join(root_dir, f'{split}.csv')\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Extract image paths and labels\n",
    "        self.image_paths = self.data['image:FILE'].values\n",
    "        self.labels = self.data['category'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Construct full image path\n",
    "        img_path = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        \n",
    "        # Read image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ],
   "id": "bb819188b4ab4655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Define CNN Model Architecture\n",
    " \n",
    "Our CNN model consists of:\n",
    "- 3 convolutional layers with ReLU activation and max pooling\n",
    "- Flatten layer to convert 2D features to 1D\n",
    "- 2 fully connected layers\n",
    "- Dropout for regularization\n",
    "- Sigmoid activation for binary classification\n"
   ],
   "id": "8867b4bfb4b5edff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n"
   ],
   "id": "1da943e1c717cf32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Data Preprocessing and Loading\n",
    "Here we:\n",
    "- Define image transformations\n",
    "- Create train and validation datasets\n",
    "- Create DataLoader instances for batch processing"
   ],
   "id": "75b5bacb6010f42a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # These random ahh values were found using ImageNet dataset, https://paperswithcode.com/dataset/imagenet\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = CatsDogsDataset(path, split='train', transform=transform)\n",
    "val_dataset = CatsDogsDataset(path, split='val', transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "c85d894559a70ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4.2 Checking our images\n",
    "Here we will load the first image of our data set and plot it along with its class"
   ],
   "id": "42184dd42e551ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img, label = train_dataset[270]\n",
    "\n",
    "# Convert tensor to numpy for visualization\n",
    "img_np = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Denormalize the image\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "img_np = std * img_np + mean\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_np)\n",
    "plt.title(f'Class: {\"Cat\" if label == 0 else \"Dog\"}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "d200e45f4d3d2274",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 5. Model Setup\n",
    "Initialize the model, loss function, and optimizer\n"
   ],
   "id": "93bc7ad89e57c5f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ],
   "id": "d35f841ee8b6cdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Training and Validation Functions\n",
    "\n",
    "Define functions for:\n",
    "- Training one epoch\n",
    "- Validating the model\n",
    "\n",
    "Both functions return loss and accuracy metrics\n"
   ],
   "id": "1b964e1a6ebe38c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return epoch_loss, accuracy\n"
   ],
   "id": "894503293aa5a590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7. Training Loop\n",
    " \n",
    "Train the model for specified number of epochs and track metrics\n",
    "\n"
   ],
   "id": "e5955d3ca8922fc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print('-' * 50)\n"
   ],
   "id": "54e846f0d5d994c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 8. Visualize Results\n",
    " \n",
    "Plot training and validation metrics over time"
   ],
   "id": "b963d6c114e6d4aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.title('Accuracy vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9041e071178cd361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Very nice\n",
    "Now you can detect all cats and dogs(As long as they are in the training set)\n",
    "\n",
    "![No cat is safe](https://i.imgur.com/1r243HR.gif)"
   ],
   "id": "6ffa55516b17f5f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "211dc5aa981a9c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

\begin{frame}[fragile]{CNN - Activation Functions}
\begin{block}{Role:}
    \begin{itemize}
        \item Introduce nonâ€‘linearity so multiple conv layers can learn complex mappings.
    \end{itemize}
\end{block}

\begin{block}{Common:}
    \begin{itemize}
        \item ReLU (Rectified Linear Unit)
        \item Leaky ReLU
        \item Sigmoid
        \item Tanh
    \end{itemize}
\end{block}

\begin{lstlisting}[language=Python, caption={Code snippet (PyTorch)}]
import torch.nn.functional as F

x = conv(input_tensor)

x = F.relu(x)              # ReLU

x = F.leaky_relu(x, 0.1)   # Leaky ReLU
\end{lstlisting}
\end{frame}  

\begin{frame}{CNN - Activation Functions}
    \begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth,height=0.95\textheight,keepaspectratio]{images/activation-functions.png}
    \end{figure}
\end{frame}